{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZxRXAXIoz4I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "STF-BooV-IlC"
      },
      "outputs": [],
      "source": [
        "!pip install setuptools==65.5.0\n",
        "#@title <font size=\"5\">← ឵឵<i>Upgrade FFmpeg to v5.0</font> { vertical-output: true }\n",
        "import os, uuid, re, IPython\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import output, drive\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os, sys, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Cloning Repositories...\", ty='twg')\n",
        "!git clone https://github.com/XniceCraft/ffmpeg-colab.git\n",
        "!chmod 755 ./ffmpeg-colab/install\n",
        "textAn(\"Installing FFmpeg...\", ty='twg')\n",
        "!./ffmpeg-colab/install\n",
        "clear_output()\n",
        "print('Installation finished!')\n",
        "!rm -fr /content/ffmpeg-colab\n",
        "!ffmpeg -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QptfCfuBZVl"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install wheel==0.38.4\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!pip install gym_super_mario_bros==7.3.0 nes_py\n",
        "!pip install tensorboard"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GMXYOzGDHqFW"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1adtvyAAMuS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from gym.wrappers import RecordVideo\n",
        "from time import time\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Taken from https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "    :param check_freq:\n",
        "    :param chk_dir: Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: Verbosity level.\n",
        "    \"\"\"\n",
        "    def __init__(self, save_freq: int, check_freq: int, chk_dir: str, verbose: int = 1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.save_freq = save_freq\n",
        "        self.chk_dir = chk_dir\n",
        "        self.save_path = os.path.join(chk_dir, 'models')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "            if self.verbose > 0:\n",
        "                print(f\"Saving current model to {os.path.join(self.chk_dir, 'models')}\")\n",
        "            self.model.save(os.path.join(self.save_path, f'iter_{self.n_calls}'))\n",
        "\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          x, y = ts2xy(load_results(self.chk_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(f\"Saving new best model to {os.path.join(self.chk_dir, 'best_model')}\")\n",
        "                  self.model.save(os.path.join(self.chk_dir, 'best_model'))\n",
        "\n",
        "        return True\n",
        "\n",
        "def startGameRand(env):    \n",
        "    fin = True\n",
        "    for step in range(100000): \n",
        "        if fin: \n",
        "            env.reset()\n",
        "        state, reward, fin, info = env.step(env.action_space.sample())\n",
        "        env.render()\n",
        "    env.close()\n",
        "\n",
        "def startGameModel(env, model):\n",
        "    state = env.reset()\n",
        "    while True: \n",
        "        action, _ = model.predict(state)\n",
        "        state, _, _, _ = env.step(action)\n",
        "        env.render()\n",
        "\n",
        "def saveGameRand(env, len = 100000, dir = './videos/'):\n",
        "    env = RecordVideo(env, dir + str(time()) + '/')\n",
        "    fin = True\n",
        "    for step in range(len): \n",
        "        if fin: \n",
        "            env.reset()\n",
        "        state, reward, fin, info = env.step(env.action_space.sample())\n",
        "    env.close()    \n",
        "\n",
        "def saveGameModel(env, model, len = 100000, dir = './videos/'):\n",
        "    env = RecordVideo(env, dir + str(time()) + '/')\n",
        "    fin = True\n",
        "    for step in range(len): \n",
        "        if fin: \n",
        "            state = env.reset()\n",
        "        action, _ = model.predict(state)\n",
        "        state, _, fin, _ = env.step(action)\n",
        "    env.close()      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei3f2hL_p7F1"
      },
      "outputs": [],
      "source": [
        "# Import the game\n",
        "import gym_super_mario_bros\n",
        "# Import the Joypad wrapper\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "# Import the SIMPLIFIED controls\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "# Import preprocessing wrappers\n",
        "# Framestack allows us to stack multiple frames together\n",
        "# GrayScaleObservation converts the frames to grayscale\n",
        "# Performance is improved by reducing the size of the frames\n",
        "from gym.wrappers import GrayScaleObservation, FrameStack\n",
        "# Import the vectorized environment\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecMonitor\n",
        "from matplotlib import pyplot as plt\n",
        "# Import PPO for training\n",
        "from stable_baselines3 import PPO\n",
        "# Import PPO for training\n",
        "from stable_baselines3 import DQN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cwuSATWsG7wm"
      },
      "source": [
        "# **Start the environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJiEQmw-qrvW"
      },
      "outputs": [],
      "source": [
        "# Generate the game environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "\n",
        "# Limit the action space to the SIMPLE_MOVEMENT\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# Note: If some error occurs in training, run this cell again.\n",
        "# Error is caused by dimensions of the observation space."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Km4qyWERE9xH"
      },
      "source": [
        "To better comprehend the training process, print the properties of the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_T33ikaqfeP"
      },
      "outputs": [],
      "source": [
        "# Actions Mario can do \n",
        "SIMPLE_MOVEMENT\n",
        "# Number of button combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzifl6jPCVh2"
      },
      "outputs": [],
      "source": [
        "# Shape of the observation space\n",
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFBW9ic7t0t3"
      },
      "outputs": [],
      "source": [
        "# Number of button combinations\n",
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IITBeZ0w9uQ"
      },
      "outputs": [],
      "source": [
        "# Do a random action\n",
        "SIMPLE_MOVEMENT[env.action_space.sample()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeeD1UnXr3Pz"
      },
      "outputs": [],
      "source": [
        "# Properties of the frame that the game will be displayed on\n",
        "env.observation_space"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "etwPrMs1HWcS"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkMvC23bCIbA"
      },
      "outputs": [],
      "source": [
        "# Grayscale reduces dimensionality\n",
        "# and improve performance (fewer data to process)\n",
        "env = GrayScaleObservation(env, keep_dim=True)\n",
        "# Note: If 'MLPPolicy' is used, then keep_dim=False\n",
        "\n",
        "# Vectorize the environment\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Stack frames\n",
        "env = VecFrameStack(env, 4)\n",
        "# Monitor your progress\n",
        "env = VecMonitor(env, \"/content/drive/MyDrive/Colab Notebooks/MarioData/PPO_altered_env/train/\")\n",
        "\n",
        "# Set the directories\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Colab Notebooks/MarioData/PPO_altered_env/train/\"\n",
        "LOG_DIR = \"/content/drive/MyDrive/Colab Notebooks/MarioData/PPO_altered_env/logs/\"\n",
        "\n",
        "# Create the callback: check every 1000 steps\n",
        "callback = SaveOnBestTrainingRewardCallback(save_freq=100000, check_freq=1000, chk_dir=CHECKPOINT_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIb5wRV8I7PT"
      },
      "source": [
        "Another way of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eCaOPbcRlv0"
      },
      "outputs": [],
      "source": [
        "# Grayscale reduces dimensionality\n",
        "# and improve performance (fewer data to process)\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "# Note: If 'MLPPolicy' is used, then keep_dim=False\n",
        "\n",
        "# Vectorize the environment\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Stack frames\n",
        "env = VecFrameStack(env, 2)\n",
        "# Monitor your progress\n",
        "env = VecMonitor(env, \"/content/drive/MyDrive/Colab Notebooks/MarioData/DQN_withMLP/train/\")\n",
        "\n",
        "# Set the directories\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Colab Notebooks/MarioData/DQN_withMLP/train/\"\n",
        "LOG_DIR = \"/content/drive/MyDrive/Colab Notebooks/MarioData/DQN_withMLP/logs/\"\n",
        "\n",
        "# Create the callback: check every 1000 steps\n",
        "callback = SaveOnBestTrainingRewardCallback(save_freq=100000, check_freq=1000, chk_dir=CHECKPOINT_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvzjk2OJR4I"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL3HjzfpJdcl"
      },
      "outputs": [],
      "source": [
        "# PPO Training \n",
        "\n",
        "# Create the model\n",
        "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.0001, n_steps=512, ent_coef=0.01, vf_coef=0.5)\n",
        "# Start the training\n",
        "model.learn(total_timesteps=1000000, log_interval=1, callback=callback)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j_dfJ07VJgCZ"
      },
      "source": [
        "Train another model with a different algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCLbBsBLJlWo"
      },
      "outputs": [],
      "source": [
        "# DQN Training \n",
        "\n",
        "# Create the model\n",
        "model = DQN('MlpPolicy', env, batch_size=192, verbose=1, learning_starts=10000, learning_rate=5e-3, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.1, train_freq=8, buffer_size=10000, tensorboard_log= LOG_DIR )\n",
        "# Start the training\n",
        "model.learn(total_timesteps=1000000, log_interval=1, callback=callback)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "akCbHz3XJv2O"
      },
      "source": [
        "# **Test the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLJDzHYPCuMe"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = PPO.load(CHECKPOINT_DIR + \"best_model\", env=env)\n",
        "# Test the model\n",
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "    action, _states = model.predict(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WqDyl3VpJzFd"
      },
      "source": [
        "# **Resume training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qso7VPwzCgF-"
      },
      "outputs": [],
      "source": [
        "# Resume a training \n",
        "\n",
        "# Load the most recent zip file\n",
        "model_dqn = DQN.load(\"/content/drive/.shortcut-targets-by-id/1-3c5hKVTtHh3lW01y3paib-wraxEX6qO/DQN_default/train/models/iter_700000\", tensorboard_log=LOG_DIR)\n",
        "# Set the environment\n",
        "model_dqn.set_env(env)\n",
        "# Resume the training\n",
        "model_dqn.learn(total_timesteps=1000001, callback=callback, log_interval=1, tb_log_name=\"DQN_1\", reset_num_timesteps=False)\n",
        "\n",
        "# Remark:\n",
        "# As the training continues, naming of the zip files may cause error, overwriting the previous written files"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M9LkVMH6J2Qb"
      },
      "source": [
        "# **Plot using TensorBoard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mYrln91FkTp"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/DQN_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwHbzXaQWDL9"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI7-W5W-Wr3H"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO6bIVY2WVG0"
      },
      "outputs": [],
      "source": [
        "# There must be a file named \"logs\" within the current directory \n",
        "# that has the log files in it\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "# Another method:\n",
        "# 1. Download the log files to your local device\n",
        "# 2. Open terminal\n",
        "# 3. Activate an environment\n",
        "# 4. Type \" %tensorboard --logdir logs \"\n",
        "# 5. Copy the link to your browser to see the plots\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qddS3cerKFfu"
      },
      "source": [
        "**Trained models:**\n",
        "\n",
        "\n",
        "1.   PPO_1 (learning rate = 0.000001)\n",
        "2.   PPO_2 (learning rate = 0.00001)\n",
        "3.   PPO_3 ('MlpPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.0001, n_steps=512, ent_coef=0.01, vf_coef=0.5)\n",
        "4.   DQN_1 ('CnnPolicy', env, batch_size=192, verbose=1, learning_starts=10000, learning_rate=5e-3, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.1, train_freq=8, buffer_size=10000, tensorboard_log= LOG_DIR )\n",
        "5.   DQN_2 ('MlpPolicy', env, batch_size=192, verbose=1, learning_starts=10000, learning_rate=5e-3, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.1, train_freq=8, buffer_size=10000, tensorboard_log= LOG_DIR )\n",
        "6.   DQN_3('CnnPolicy', env, batch_size=256, verbose=1,learning_starts=10000 learning_rate=1e-4, exploration_fraction=0.2, exploration_initial_eps=1.0,exploration_final_eps=0.05, train_freq=16, buffer_size=50000, tensorboard_log=LOG_DIR)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
